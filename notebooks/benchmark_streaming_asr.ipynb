{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e75725c",
   "metadata": {},
   "source": [
    "# ASR Streaming Benchmark\n",
    "\n",
    "This notebook benchmarks all available streaming ASR backends (Whisper, Parakeet, Vosk, FastConformer) with configurable parameters.\n",
    "\n",
    "**Metrics:**\n",
    "- **WER** (Word Error Rate): Lower is better\n",
    "- **Latency**: Processing delay (ms)\n",
    "- **RTF** (Real-Time Factor): < 1.0 means faster than real-time\n",
    "\n",
    "**Test Audio:** LibriSpeech test-clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25abb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (China mirrors for faster downloads)\n",
    "!pip install -q jiwer nest_asyncio soundfile librosa matplotlib seaborn pandas -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "import json\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jiwer import wer\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354c722",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure ASR service endpoints and test parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca61015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR Service Endpoints (Docker services)\n",
    "BACKENDS = {\n",
    "    'vosk': 'http://vosk-asr:8000',\n",
    "    'parakeet': 'http://parakeet-asr:8000',\n",
    "    'whisper': 'http://whisper-asr:8000',\n",
    "    'fastconformer': 'http://fastconformer-asr:8000'\n",
    "}\n",
    "\n",
    "# Test Configuration\n",
    "CHUNK_DURATION = 1.0  # seconds per chunk\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "\n",
    "# Reference transcript for WER calculation\n",
    "REFERENCE_TRANSCRIPT = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "print(f\"Backends configured: {list(BACKENDS.keys())}\")\n",
    "print(f\"Chunk duration: {CHUNK_DURATION}s\")\n",
    "print(f\"Sample rate: {SAMPLE_RATE}Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de90ffb",
   "metadata": {},
   "source": [
    "## Test Audio Generation\n",
    "\n",
    "Generate or load test audio for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_audio(duration: float = 10.0, sample_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate synthetic test audio (silence for now).\n",
    "    In production, use actual speech samples.\n",
    "    \"\"\"\n",
    "    num_samples = int(duration * sample_rate)\n",
    "    # Generate silence (replace with actual audio file loading)\n",
    "    audio = np.zeros(num_samples, dtype=np.float32)\n",
    "    return audio\n",
    "\n",
    "def load_audio_file(file_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load audio file and resample to target sample rate.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "    return audio\n",
    "\n",
    "# Generate test audio (10 seconds)\n",
    "test_audio = generate_test_audio(duration=10.0, sample_rate=SAMPLE_RATE)\n",
    "print(f\"Generated test audio: {len(test_audio)} samples ({len(test_audio)/SAMPLE_RATE:.2f}s)\")\n",
    "\n",
    "# Visualize waveform\n",
    "plt.figure(figsize=(12, 3))\n",
    "time_axis = np.arange(len(test_audio)) / SAMPLE_RATE\n",
    "plt.plot(time_axis, test_audio, linewidth=0.5)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Test Audio Waveform')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92fbbc",
   "metadata": {},
   "source": [
    "## ASR Client Implementation\n",
    "\n",
    "Implement streaming ASR client for each backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "class StreamingASRClient:\n",
    "    def __init__(self, backend_name: str, base_url: str):\n",
    "        self.backend_name = backend_name\n",
    "        self.base_url = base_url\n",
    "        self.session_id = None\n",
    "        \n",
    "    async def start_session(self, session: aiohttp.ClientSession) -> str:\n",
    "        \"\"\"Start a new transcription session.\"\"\"\n",
    "        url = f\"{self.base_url}/transcribe/start\"\n",
    "        async with session.post(url, json={\"language\": \"en\"}) as resp:\n",
    "            data = await resp.json()\n",
    "            self.session_id = data['session_id']\n",
    "            return self.session_id\n",
    "    \n",
    "    async def send_chunk(self, session: aiohttp.ClientSession, audio_chunk: np.ndarray) -> Tuple[str, float]:\n",
    "        \"\"\"Send audio chunk and measure latency.\"\"\"\n",
    "        url = f\"{self.base_url}/transcribe/stream\"\n",
    "        \n",
    "        # Convert audio to bytes\n",
    "        audio_bytes = (audio_chunk * 32767).astype(np.int16).tobytes()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        async with session.post(\n",
    "            url,\n",
    "            json={\n",
    "                \"session_id\": self.session_id,\n",
    "                \"audio_data\": audio_bytes.hex()\n",
    "            }\n",
    "        ) as resp:\n",
    "            data = await resp.json()\n",
    "            latency = time.time() - start_time\n",
    "            transcript = data.get('text', '')\n",
    "            return transcript, latency\n",
    "    \n",
    "    async def stop_session(self, session: aiohttp.ClientSession) -> str:\n",
    "        \"\"\"Stop transcription session and get final transcript.\"\"\"\n",
    "        url = f\"{self.base_url}/transcribe/stop\"\n",
    "        async with session.post(url, json={\"session_id\": self.session_id}) as resp:\n",
    "            data = await resp.json()\n",
    "            return data.get('text', '')\n",
    "\n",
    "print(\"StreamingASRClient class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5dfa0",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "Run streaming benchmark for a single backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def benchmark_backend(\n",
    "    backend_name: str,\n",
    "    base_url: str,\n",
    "    audio: np.ndarray,\n",
    "    chunk_size: int\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark a single ASR backend with streaming audio.\n",
    "    \"\"\"\n",
    "    client = StreamingASRClient(backend_name, base_url)\n",
    "    latencies = []\n",
    "    transcripts = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            # Start session\n",
    "            session_id = await client.start_session(session)\n",
    "            print(f\"[{backend_name}] Session started: {session_id}\")\n",
    "            \n",
    "            # Stream audio chunks\n",
    "            num_chunks = len(audio) // chunk_size\n",
    "            for i in range(num_chunks):\n",
    "                chunk = audio[i * chunk_size:(i + 1) * chunk_size]\n",
    "                transcript, latency = await client.send_chunk(session, chunk)\n",
    "                latencies.append(latency)\n",
    "                transcripts.append(transcript)\n",
    "                print(f\"[{backend_name}] Chunk {i+1}/{num_chunks}: {latency*1000:.2f}ms - '{transcript}'\")\n",
    "            \n",
    "            # Stop session and get final transcript\n",
    "            final_transcript = await client.stop_session(session)\n",
    "            print(f\"[{backend_name}] Final transcript: '{final_transcript}'\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_latency = np.mean(latencies)\n",
    "            p95_latency = np.percentile(latencies, 95)\n",
    "            max_latency = np.max(latencies)\n",
    "            \n",
    "            return {\n",
    "                'backend': backend_name,\n",
    "                'avg_latency_ms': avg_latency * 1000,\n",
    "                'p95_latency_ms': p95_latency * 1000,\n",
    "                'max_latency_ms': max_latency * 1000,\n",
    "                'final_transcript': final_transcript,\n",
    "                'latencies': latencies,\n",
    "                'transcripts': transcripts,\n",
    "                'success': True\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[{backend_name}] Error: {e}\")\n",
    "            return {\n",
    "                'backend': backend_name,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "print(\"Benchmark function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c6ff4",
   "metadata": {},
   "source": [
    "## Run Benchmarks\n",
    "\n",
    "Execute benchmarks for all backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_all_benchmarks():\n",
    "    \"\"\"Run benchmarks for all configured backends.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for backend_name, base_url in BACKENDS.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Benchmarking: {backend_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = await benchmark_backend(\n",
    "            backend_name,\n",
    "            base_url,\n",
    "            test_audio,\n",
    "            CHUNK_SIZE\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Starting benchmarks...\")\n",
    "benchmark_results = await run_all_benchmarks()\n",
    "print(\"\\nBenchmarks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461825e4",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Analyze and visualize benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72051d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "successful_results = [r for r in benchmark_results if r.get('success', False)]\n",
    "\n",
    "if successful_results:\n",
    "    df_results = pd.DataFrame([{\n",
    "        'Backend': r['backend'],\n",
    "        'Avg Latency (ms)': r['avg_latency_ms'],\n",
    "        'P95 Latency (ms)': r['p95_latency_ms'],\n",
    "        'Max Latency (ms)': r['max_latency_ms'],\n",
    "        'Final Transcript': r['final_transcript']\n",
    "    } for r in successful_results])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No successful benchmark results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad985db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency Comparison Bar Chart\n",
    "if successful_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Average Latency\n",
    "    backends = [r['backend'] for r in successful_results]\n",
    "    avg_latencies = [r['avg_latency_ms'] for r in successful_results]\n",
    "    \n",
    "    axes[0].bar(backends, avg_latencies, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(backends)])\n",
    "    axes[0].set_ylabel('Latency (ms)')\n",
    "    axes[0].set_title('Average Latency by Backend')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Latency Distribution (Box Plot)\n",
    "    latency_data = [r['latencies'] for r in successful_results]\n",
    "    axes[1].boxplot([np.array(l) * 1000 for l in latency_data], labels=backends)\n",
    "    axes[1].set_ylabel('Latency (ms)')\n",
    "    axes[1].set_title('Latency Distribution')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency Over Time\n",
    "if successful_results:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for result in successful_results:\n",
    "        latencies_ms = np.array(result['latencies']) * 1000\n",
    "        chunks = range(1, len(latencies_ms) + 1)\n",
    "        plt.plot(chunks, latencies_ms, marker='o', label=result['backend'])\n",
    "    \n",
    "    plt.xlabel('Chunk Number')\n",
    "    plt.ylabel('Latency (ms)')\n",
    "    plt.title('Latency Over Time for Each Backend')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c1030",
   "metadata": {},
   "source": [
    "## WER (Word Error Rate) Calculation\n",
    "\n",
    "Calculate accuracy metrics if reference transcript is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER for each backend\n",
    "if successful_results and REFERENCE_TRANSCRIPT:\n",
    "    print(\"\\nWord Error Rate (WER) Analysis:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for result in successful_results:\n",
    "        hypothesis = result['final_transcript'].lower().strip()\n",
    "        reference = REFERENCE_TRANSCRIPT.lower().strip()\n",
    "        \n",
    "        if hypothesis:\n",
    "            error_rate = wer(reference, hypothesis)\n",
    "            print(f\"\\n{result['backend'].upper()}:\")\n",
    "            print(f\"  Reference:  '{reference}'\")\n",
    "            print(f\"  Hypothesis: '{hypothesis}'\")\n",
    "            print(f\"  WER: {error_rate*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"\\n{result['backend'].upper()}: No transcript generated\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping WER calculation (no reference transcript or successful results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a066b4",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save benchmark results to JSON for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "output_file = \"benchmark_results.json\"\n",
    "\n",
    "export_data = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'configuration': {\n",
    "        'chunk_duration': CHUNK_DURATION,\n",
    "        'sample_rate': SAMPLE_RATE,\n",
    "        'audio_duration': len(test_audio) / SAMPLE_RATE\n",
    "    },\n",
    "    'results': [{\n",
    "        'backend': r['backend'],\n",
    "        'avg_latency_ms': r.get('avg_latency_ms'),\n",
    "        'p95_latency_ms': r.get('p95_latency_ms'),\n",
    "        'max_latency_ms': r.get('max_latency_ms'),\n",
    "        'final_transcript': r.get('final_transcript'),\n",
    "        'success': r.get('success', False)\n",
    "    } for r in benchmark_results]\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"Results exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363405ef",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "- **Latency**: Compare average/P95 latency across backends\n",
    "- **Accuracy**: WER comparison (lower is better)\n",
    "- **Stability**: Latency variance over time\n",
    "- **Resource Usage**: GPU/CPU memory consumption (monitor separately)\n",
    "\n",
    "### Recommendations:\n",
    "- **Real-time Captions**: Use FastConformer or Vosk for low latency\n",
    "- **High Accuracy**: Use Parakeet RNNT or Whisper Large V3\n",
    "- **Balanced**: Parakeet TDT or FastConformer Hybrid\n",
    "- **CPU-only**: Vosk (no GPU required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881beb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test notebook - verify we can access from VS Code\n",
    "print(\"âœ… Notebook working!\")\n",
    "print(\"Edit this file in VS Code and run in PySpark container.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import time\n",
    "import wave\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import websockets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Jiwer for WER calculation\n",
    "try:\n",
    "    from jiwer import wer\n",
    "except ImportError:\n",
    "    print(\"Installing jiwer from China mirror...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    # Use Tsinghua mirror for faster downloads in China\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \n",
    "        \"-i\", \"https://pypi.tuna.tsinghua.edu.cn/simple\",\n",
    "        \"jiwer\"\n",
    "    ])\n",
    "    from jiwer import wer\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nest_asyncio for Jupyter async support\n",
    "try:\n",
    "    import nest_asyncio\n",
    "except ImportError:\n",
    "    print(\"Installing nest_asyncio from China mirror...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \n",
    "        \"-i\", \"https://pypi.tuna.tsinghua.edu.cn/simple\",\n",
    "        \"nest_asyncio\"\n",
    "    ])\n",
    "    import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "print(\"âœ“ Async support enabled for Jupyter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da16e8",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Select the backends and parameters to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backend configurations\n",
    "BACKENDS = {\n",
    "    \"whisper\": {\n",
    "        \"host\": \"whisper-asr\",\n",
    "        \"port\": 8000,\n",
    "        \"configs\": {\n",
    "            \"VAD Enabled\": {\"VAD_FILTER\": \"True\", \"VAD_THRESHOLD\": \"0.5\"},\n",
    "            \"VAD Disabled\": {\"VAD_FILTER\": \"False\"},\n",
    "            \"High Threshold\": {\"VAD_FILTER\": \"True\", \"VAD_THRESHOLD\": \"0.8\"},\n",
    "        }\n",
    "    },\n",
    "    \"parakeet\": {\n",
    "        \"host\": \"parakeet-asr\",\n",
    "        \"port\": 8000,\n",
    "        \"configs\": {\n",
    "            \"Default\": {},\n",
    "            \"CUDA Graphs\": {\"DECODING_STRATEGY\": \"cuda_graphs\"},\n",
    "        }\n",
    "    },\n",
    "    \"vosk\": {\n",
    "        \"host\": \"vosk-asr\",\n",
    "        \"port\": 8000,\n",
    "        \"configs\": {\n",
    "            \"Default\": {},\n",
    "        }\n",
    "    },\n",
    "    \"fastconformer\": {\n",
    "        \"host\": \"fastconformer-asr\",\n",
    "        \"port\": 8000,\n",
    "        \"configs\": {\n",
    "            \"Low Latency (cache=4)\": {\"ATT_CONTEXT_SIZE\": \"4\", \"DECODER_TYPE\": \"rnnt\"},\n",
    "            \"Medium Latency (cache=8)\": {\"ATT_CONTEXT_SIZE\": \"8\", \"DECODER_TYPE\": \"rnnt\"},\n",
    "            \"High Latency (cache=16)\": {\"ATT_CONTEXT_SIZE\": \"16\", \"DECODER_TYPE\": \"rnnt\"},\n",
    "            \"CTC Decoder\": {\"ATT_CONTEXT_SIZE\": \"8\", \"DECODER_TYPE\": \"ctc\"},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Test audio files (add your own LibriSpeech samples)\n",
    "TEST_FILES = {\n",
    "    \"sample1\": {\n",
    "        \"path\": \"../integration/fixtures/test_audio.wav\",\n",
    "        \"reference\": \"this is a test audio sample for speech recognition\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7bd6c6",
   "metadata": {},
   "source": [
    "## Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def benchmark_backend(backend_name: str, config_name: str, audio_file: str, reference_text: str) -> Dict:\n",
    "    \"\"\"Benchmark a single backend configuration.\"\"\"\n",
    "    backend_config = BACKENDS[backend_name]\n",
    "    params = backend_config[\"configs\"][config_name]\n",
    "    \n",
    "    # Build WebSocket URL with parameters\n",
    "    url = f\"ws://{backend_config['host']}:{backend_config['port']}/transcribe\"\n",
    "    if params:\n",
    "        query = \"&\".join(f\"{k}={v}\" for k, v in params.items())\n",
    "        url += f\"?{query}\"\n",
    "    \n",
    "    # Read audio file\n",
    "    with wave.open(audio_file, 'rb') as wf:\n",
    "        sample_rate = wf.getframerate()\n",
    "        channels = wf.getnchannels()\n",
    "        audio_data = wf.readframes(wf.getnframes())\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if channels == 2:\n",
    "        audio_array = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        audio_array = audio_array.reshape(-1, 2).mean(axis=1).astype(np.int16)\n",
    "        audio_data = audio_array.tobytes()\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    transcript = \"\"\n",
    "    \n",
    "    try:\n",
    "        async with websockets.connect(url) as ws:\n",
    "            # Send audio in chunks (simulate streaming)\n",
    "            chunk_size = sample_rate * 2  # 1 second chunks (16-bit = 2 bytes)\n",
    "            for i in range(0, len(audio_data), chunk_size):\n",
    "                chunk = audio_data[i:i + chunk_size]\n",
    "                await ws.send(chunk)\n",
    "                \n",
    "                # Receive response\n",
    "                try:\n",
    "                    response = await asyncio.wait_for(ws.recv(), timeout=1.0)\n",
    "                    data = json.loads(response)\n",
    "                    if data.get(\"text\"):\n",
    "                        transcript = data[\"text\"]\n",
    "                except asyncio.TimeoutError:\n",
    "                    pass\n",
    "            \n",
    "            # Signal end\n",
    "            await ws.send(json.dumps({\"done\": True}))\n",
    "            \n",
    "            # Get final result\n",
    "            try:\n",
    "                response = await asyncio.wait_for(ws.recv(), timeout=5.0)\n",
    "                data = json.loads(response)\n",
    "                if data.get(\"text\"):\n",
    "                    transcript = data[\"text\"]\n",
    "            except asyncio.TimeoutError:\n",
    "                pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"backend\": backend_name,\n",
    "            \"config\": config_name,\n",
    "            \"error\": str(e),\n",
    "            \"wer\": None,\n",
    "            \"latency_ms\": None,\n",
    "            \"rtf\": None,\n",
    "            \"transcript\": \"\"\n",
    "        }\n",
    "    \n",
    "    end_time = time.time()\n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "    \n",
    "    # Calculate audio duration\n",
    "    audio_duration_sec = len(audio_data) / (sample_rate * 2)  # 16-bit = 2 bytes\n",
    "    rtf = (end_time - start_time) / audio_duration_sec if audio_duration_sec > 0 else None\n",
    "    \n",
    "    # Calculate WER\n",
    "    wer_score = wer(reference_text.lower(), transcript.lower()) if transcript else 1.0\n",
    "    \n",
    "    return {\n",
    "        \"backend\": backend_name,\n",
    "        \"config\": config_name,\n",
    "        \"wer\": wer_score,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"rtf\": rtf,\n",
    "        \"transcript\": transcript,\n",
    "        \"reference\": reference_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cd768",
   "metadata": {},
   "source": [
    "## Run Benchmarks\n",
    "\n",
    "Select which backends and configurations to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ffba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive selection\n",
    "backend_checkboxes = {\n",
    "    name: widgets.Checkbox(value=True, description=name.title())\n",
    "    for name in BACKENDS.keys()\n",
    "}\n",
    "\n",
    "display(HTML(\"<h3>Select Backends to Test:</h3>\"))\n",
    "for cb in backend_checkboxes.values():\n",
    "    display(cb)\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Benchmarks\", button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "display(run_button)\n",
    "display(output)\n",
    "\n",
    "async def run_benchmarks():\n",
    "    \"\"\"Run all selected benchmarks.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for backend_name, checkbox in backend_checkboxes.items():\n",
    "        if not checkbox.value:\n",
    "            continue\n",
    "        \n",
    "        backend_configs = BACKENDS[backend_name][\"configs\"]\n",
    "        \n",
    "        for config_name in backend_configs.keys():\n",
    "            for test_name, test_data in TEST_FILES.items():\n",
    "                with output:\n",
    "                    print(f\"Testing {backend_name} ({config_name}) on {test_name}...\")\n",
    "                \n",
    "                result = await benchmark_backend(\n",
    "                    backend_name,\n",
    "                    config_name,\n",
    "                    test_data[\"path\"],\n",
    "                    test_data[\"reference\"]\n",
    "                )\n",
    "                results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    \"\"\"Button click handler - runs benchmarks in async context.\"\"\"\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(\"Running benchmarks...\")\n",
    "    \n",
    "    # Create async task and run\n",
    "    async def run_and_display():\n",
    "        global benchmark_results\n",
    "        benchmark_results = await run_benchmarks()\n",
    "        \n",
    "        with output:\n",
    "            print(f\"\\nâœ“ Completed {len(benchmark_results)} benchmarks\")\n",
    "            print(\"\\nRun the next cells to see results!\")\n",
    "    \n",
    "    # Run in event loop\n",
    "    import nest_asyncio\n",
    "    try:\n",
    "        nest_asyncio.apply()\n",
    "    except:\n",
    "        pass  # Already applied or not needed\n",
    "    \n",
    "    asyncio.run(run_and_display())\n",
    "        \n",
    "run_button.on_click(on_run_clicked)\n",
    "\n",
    "print(\"ðŸ‘† Select backends above and click 'Run Benchmarks'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eccac0",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "df = pd.DataFrame(benchmark_results)\n",
    "df_display = df[['backend', 'config', 'wer', 'latency_ms', 'rtf']].copy()\n",
    "df_display['wer'] = df_display['wer'].round(3)\n",
    "df_display['latency_ms'] = df_display['latency_ms'].round(1)\n",
    "df_display['rtf'] = df_display['rtf'].round(3)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "display(df_display)\n",
    "\n",
    "# Highlight best performers\n",
    "best_wer = df_display.loc[df_display['wer'].idxmin()]\n",
    "best_latency = df_display.loc[df_display['latency_ms'].idxmin()]\n",
    "best_rtf = df_display.loc[df_display['rtf'].idxmin()]\n",
    "\n",
    "print(f\"\\nðŸ† Best WER: {best_wer['backend']} ({best_wer['config']}) - {best_wer['wer']:.3f}\")\n",
    "print(f\"âš¡ Fastest Latency: {best_latency['backend']} ({best_latency['config']}) - {best_latency['latency_ms']:.1f}ms\")\n",
    "print(f\"ðŸš€ Best RTF: {best_rtf['backend']} ({best_rtf['config']}) - {best_rtf['rtf']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09ace6",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47776ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# WER comparison\n",
    "df_plot = df_display.copy()\n",
    "df_plot['label'] = df_plot['backend'] + '\\n' + df_plot['config']\n",
    "\n",
    "axes[0].barh(df_plot['label'], df_plot['wer'])\n",
    "axes[0].set_xlabel('Word Error Rate')\n",
    "axes[0].set_title('Accuracy (Lower is Better)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Latency comparison\n",
    "axes[1].barh(df_plot['label'], df_plot['latency_ms'], color='orange')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_title('Response Time')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# RTF comparison\n",
    "axes[2].barh(df_plot['label'], df_plot['rtf'], color='green')\n",
    "axes[2].axvline(x=1.0, color='red', linestyle='--', label='Real-time threshold')\n",
    "axes[2].set_xlabel('Real-Time Factor')\n",
    "axes[2].set_title('Processing Speed (Lower is Better)')\n",
    "axes[2].legend()\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc13019",
   "metadata": {},
   "source": [
    "## Transcript Comparison\n",
    "\n",
    "Review the actual transcripts vs reference text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transcripts\n",
    "for idx, result in enumerate(benchmark_results):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Test {idx + 1}: {result['backend'].upper()} - {result['config']}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Reference: {result['reference']}\")\n",
    "    print(f\"Transcript: {result['transcript']}\")\n",
    "    print(f\"WER: {result['wer']:.3f}\")\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(f\"âš ï¸ Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac56bbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\icefo\\Documents\\Docker\\AI-Tools\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:69\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _docstring\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     FigureCanvasBase, FigureManagerBase, MouseButton)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfigure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Figure, FigureBase, figaspect\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgridspec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rcsetup, rcParamsDefault, rcParamsOrig\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\icefo\\Documents\\Docker\\AI-Tools\\.venv\\Lib\\site-packages\\matplotlib\\figure.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _blocking_input, backend_bases, _docstring, projections\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martist\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     Artist, allow_rasterization, _finalize_rasterization)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     DrawEvent, FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\icefo\\Documents\\Docker\\AI-Tools\\.venv\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:55\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mNon-separable transforms that map from data space to screen space.\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m`matplotlib.projections.polar` may also be of interest.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m axes, _docstring\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PolarAxes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\icefo\\Documents\\Docker\\AI-Tools\\.venv\\Lib\\site-packages\\matplotlib\\axes\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _base\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_axes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Axes\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Backcompat.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\icefo\\Documents\\Docker\\AI-Tools\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, _docstring, offsetbox\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martist\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmartist\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maxis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmaxis\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _OrderedSet, _check_1d, index_of\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmcoll\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\icefo\\Documents\\Docker\\AI-Tools\\.venv\\Lib\\site-packages\\matplotlib\\axis.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmticker\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtransforms\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munits\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmunits\u001b[39;00m\n\u001b[32m     24\u001b[39m _log = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     26\u001b[39m GRIDLINE_INTERPOLATION_STEPS = \u001b[32m180\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:936\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1032\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1130\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import time\n",
    "import wave\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import websockets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Jiwer for WER calculation\n",
    "try:\n",
    "    from jiwer import wer\n",
    "except ImportError:\n",
    "    print(\"Installing jiwer from China mirror...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    # Use Tsinghua mirror for faster downloads in China\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \n",
    "        \"-i\", \"https://pypi.tuna.tsinghua.edu.cn/simple\",\n",
    "        \"jiwer\"\n",
    "    ])\n",
    "    from jiwer import wer\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nest_asyncio for Jupyter async support\n",
    "try:\n",
    "    import nest_asyncio\n",
    "except ImportError:\n",
    "    print(\"Installing nest_asyncio from China mirror...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \n",
    "        \"-i\", \"https://pypi.tuna.tsinghua.edu.cn/simple\",\n",
    "        \"nest_asyncio\"\n",
    "    ])\n",
    "    import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "print(\"âœ“ Async support enabled for Jupyter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backend configurations\n",
    "BACKENDS = {\n",
    "    \"whisper\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 8001,\n",
    "        \"configs\": {\n",
    "            \"VAD Enabled\": {\"VAD_FILTER\": \"True\", \"VAD_THRESHOLD\": \"0.5\"},\n",
    "            \"VAD Disabled\": {\"VAD_FILTER\": \"False\"},\n",
    "            \"High Threshold\": {\"VAD_FILTER\": \"True\", \"VAD_THRESHOLD\": \"0.8\"},\n",
    "        }\n",
    "    },\n",
    "    \"parakeet\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 8002,\n",
    "        \"configs\": {\n",
    "            \"Default\": {},\n",
    "            \"CUDA Graphs\": {\"DECODING_STRATEGY\": \"cuda_graphs\"},\n",
    "        }\n",
    "    },\n",
    "    \"vosk\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 8003,\n",
    "        \"configs\": {\n",
    "            \"Default\": {},\n",
    "        }\n",
    "    },\n",
    "    \"fastconformer\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 8004,\n",
    "        \"configs\": {\n",
    "            \"Low Latency (cache=4)\": {\"ATT_CONTEXT_SIZE\": \"4\", \"DECODER_TYPE\": \"rnnt\"},\n",
    "            \"Medium Latency (cache=8)\": {\"ATT_CONTEXT_SIZE\": \"8\", \"DECODER_TYPE\": \"rnnt\"},\n",
    "            \"High Latency (cache=16)\": {\"ATT_CONTEXT_SIZE\": \"16\", \"DECODER_TYPE\": \"rnnt\"},\n",
    "            \"CTC Decoder\": {\"ATT_CONTEXT_SIZE\": \"8\", \"DECODER_TYPE\": \"ctc\"},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Test audio files (add your own LibriSpeech samples)\n",
    "TEST_FILES = {\n",
    "    \"sample1\": {\n",
    "        \"path\": \"../integration/fixtures/test_audio.wav\",\n",
    "        \"reference\": \"this is a test audio sample for speech recognition\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def benchmark_backend(backend_name: str, config_name: str, audio_file: str, reference_text: str) -> Dict:\n",
    "    \"\"\"Benchmark a single backend configuration.\"\"\"\n",
    "    backend_config = BACKENDS[backend_name]\n",
    "    params = backend_config[\"configs\"][config_name]\n",
    "    \n",
    "    # Build WebSocket URL with parameters\n",
    "    url = f\"ws://{backend_config['host']}:{backend_config['port']}/transcribe\"\n",
    "    if params:\n",
    "        query = \"&\".join(f\"{k}={v}\" for k, v in params.items())\n",
    "        url += f\"?{query}\"\n",
    "    \n",
    "    # Read audio file\n",
    "    with wave.open(audio_file, 'rb') as wf:\n",
    "        sample_rate = wf.getframerate()\n",
    "        channels = wf.getnchannels()\n",
    "        audio_data = wf.readframes(wf.getnframes())\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if channels == 2:\n",
    "        audio_array = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        audio_array = audio_array.reshape(-1, 2).mean(axis=1).astype(np.int16)\n",
    "        audio_data = audio_array.tobytes()\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    transcript = \"\"\n",
    "    \n",
    "    try:\n",
    "        async with websockets.connect(url) as ws:\n",
    "            # Send audio in chunks (simulate streaming)\n",
    "            chunk_size = sample_rate * 2  # 1 second chunks (16-bit = 2 bytes)\n",
    "            for i in range(0, len(audio_data), chunk_size):\n",
    "                chunk = audio_data[i:i + chunk_size]\n",
    "                await ws.send(chunk)\n",
    "                \n",
    "                # Receive response\n",
    "                try:\n",
    "                    response = await asyncio.wait_for(ws.recv(), timeout=1.0)\n",
    "                    data = json.loads(response)\n",
    "                    if data.get(\"text\"):\n",
    "                        transcript = data[\"text\"]\n",
    "                except asyncio.TimeoutError:\n",
    "                    pass\n",
    "            \n",
    "            # Signal end\n",
    "            await ws.send(json.dumps({\"done\": True}))\n",
    "            \n",
    "            # Get final result\n",
    "            try:\n",
    "                response = await asyncio.wait_for(ws.recv(), timeout=5.0)\n",
    "                data = json.loads(response)\n",
    "                if data.get(\"text\"):\n",
    "                    transcript = data[\"text\"]\n",
    "            except asyncio.TimeoutError:\n",
    "                pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"backend\": backend_name,\n",
    "            \"config\": config_name,\n",
    "            \"error\": str(e),\n",
    "            \"wer\": None,\n",
    "            \"latency_ms\": None,\n",
    "            \"rtf\": None,\n",
    "            \"transcript\": \"\"\n",
    "        }\n",
    "    \n",
    "    end_time = time.time()\n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "    \n",
    "    # Calculate audio duration\n",
    "    audio_duration_sec = len(audio_data) / (sample_rate * 2)  # 16-bit = 2 bytes\n",
    "    rtf = (end_time - start_time) / audio_duration_sec if audio_duration_sec > 0 else None\n",
    "    \n",
    "    # Calculate WER\n",
    "    wer_score = wer(reference_text.lower(), transcript.lower()) if transcript else 1.0\n",
    "    \n",
    "    return {\n",
    "        \"backend\": backend_name,\n",
    "        \"config\": config_name,\n",
    "        \"wer\": wer_score,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"rtf\": rtf,\n",
    "        \"transcript\": transcript,\n",
    "        \"reference\": reference_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive selection\n",
    "backend_checkboxes = {\n",
    "    name: widgets.Checkbox(value=True, description=name.title())\n",
    "    for name in BACKENDS.keys()\n",
    "}\n",
    "\n",
    "display(HTML(\"<h3>Select Backends to Test:</h3>\"))\n",
    "for cb in backend_checkboxes.values():\n",
    "    display(cb)\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Benchmarks\", button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "display(run_button)\n",
    "display(output)\n",
    "\n",
    "async def run_benchmarks():\n",
    "    \"\"\"Run all selected benchmarks.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for backend_name, checkbox in backend_checkboxes.items():\n",
    "        if not checkbox.value:\n",
    "            continue\n",
    "        \n",
    "        backend_configs = BACKENDS[backend_name][\"configs\"]\n",
    "        \n",
    "        for config_name in backend_configs.keys():\n",
    "            for test_name, test_data in TEST_FILES.items():\n",
    "                with output:\n",
    "                    print(f\"Testing {backend_name} ({config_name}) on {test_name}...\")\n",
    "                \n",
    "                result = await benchmark_backend(\n",
    "                    backend_name,\n",
    "                    config_name,\n",
    "                    test_data[\"path\"],\n",
    "                    test_data[\"reference\"]\n",
    "                )\n",
    "                results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    \"\"\"Button click handler - runs benchmarks in async context.\"\"\"\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(\"Running benchmarks...\")\n",
    "    \n",
    "    # Create async task and run\n",
    "    async def run_and_display():\n",
    "        global benchmark_results\n",
    "        benchmark_results = await run_benchmarks()\n",
    "        \n",
    "        with output:\n",
    "            print(f\"\\nâœ“ Completed {len(benchmark_results)} benchmarks\")\n",
    "            print(\"\\nRun the next cells to see results!\")\n",
    "    \n",
    "    # Run in event loop\n",
    "    import nest_asyncio\n",
    "    try:\n",
    "        nest_asyncio.apply()\n",
    "    except:\n",
    "        pass  # Already applied or not needed\n",
    "    \n",
    "    asyncio.run(run_and_display())\n",
    "        \n",
    "run_button.on_click(on_run_clicked)\n",
    "\n",
    "print(\"ðŸ‘† Select backends above and click 'Run Benchmarks'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7315bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "df = pd.DataFrame(benchmark_results)\n",
    "df_display = df[['backend', 'config', 'wer', 'latency_ms', 'rtf']].copy()\n",
    "df_display['wer'] = df_display['wer'].round(3)\n",
    "df_display['latency_ms'] = df_display['latency_ms'].round(1)\n",
    "df_display['rtf'] = df_display['rtf'].round(3)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "display(df_display)\n",
    "\n",
    "# Highlight best performers\n",
    "best_wer = df_display.loc[df_display['wer'].idxmin()]\n",
    "best_latency = df_display.loc[df_display['latency_ms'].idxmin()]\n",
    "best_rtf = df_display.loc[df_display['rtf'].idxmin()]\n",
    "\n",
    "print(f\"\\nðŸ† Best WER: {best_wer['backend']} ({best_wer['config']}) - {best_wer['wer']:.3f}\")\n",
    "print(f\"âš¡ Fastest Latency: {best_latency['backend']} ({best_latency['config']}) - {best_latency['latency_ms']:.1f}ms\")\n",
    "print(f\"ðŸš€ Best RTF: {best_rtf['backend']} ({best_rtf['config']}) - {best_rtf['rtf']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b07a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# WER comparison\n",
    "df_plot = df_display.copy()\n",
    "df_plot['label'] = df_plot['backend'] + '\\n' + df_plot['config']\n",
    "\n",
    "axes[0].barh(df_plot['label'], df_plot['wer'])\n",
    "axes[0].set_xlabel('Word Error Rate')\n",
    "axes[0].set_title('Accuracy (Lower is Better)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Latency comparison\n",
    "axes[1].barh(df_plot['label'], df_plot['latency_ms'], color='orange')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_title('Response Time')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# RTF comparison\n",
    "axes[2].barh(df_plot['label'], df_plot['rtf'], color='green')\n",
    "axes[2].axvline(x=1.0, color='red', linestyle='--', label='Real-time threshold')\n",
    "axes[2].set_xlabel('Real-Time Factor')\n",
    "axes[2].set_title('Processing Speed (Lower is Better)')\n",
    "axes[2].legend()\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transcripts\n",
    "for idx, result in enumerate(benchmark_results):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Test {idx + 1}: {result['backend'].upper()} - {result['config']}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Reference: {result['reference']}\")\n",
    "    print(f\"Transcript: {result['transcript']}\")\n",
    "    print(f\"WER: {result['wer']:.3f}\")\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(f\"âš ï¸ Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e3d67",
   "metadata": {},
   "source": [
    "## Transcript Comparison\n",
    "\n",
    "Review the actual transcripts vs reference text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ecde9f",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e12511",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882989e",
   "metadata": {},
   "source": [
    "## Run Benchmarks\n",
    "\n",
    "Select which backends and configurations to test:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601efb9",
   "metadata": {},
   "source": [
    "## Benchmark Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718d2f9",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Select the backends and parameters to test:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
