# Docker Compose configuration for AI-Tools
# Define all services (containers) to be orchestrated
#
# ============== CHINA MIRROR SUPPORT ==============
# China-friendly mirrors are ENABLED BY DEFAULT for:
# - APT packages (Aliyun mirror for Ubuntu/Debian)
# - PyPI packages (Aliyun mirror for pip)
# - HuggingFace models (hf-mirror.com)
#
# Optional: Configure Docker Hub mirror (in Docker daemon.json):
#    {
#      "registry-mirrors": [
#        "https://docker.nju.edu.cn",
#        "https://hub-mirror.c.163.com",
#        "https://mirror.ccs.tencentyun.com"
#      ]
#    }
# =================================================

# ============== MODEL CONFIGURATION ==============
# Parakeet models (separate streaming vs offline for optimal performance):
#   nvidia/parakeet-tdt-1.1b   - TDT: Fast streaming, good for real-time (FP16 OK, ~4GB VRAM)
#   nvidia/parakeet-rnnt-1.1b  - RNNT: Best accuracy, for offline transcription (FP32 supported, ~8GB VRAM)
#   nvidia/parakeet-ctc-1.1b   - CTC: Balanced speed/accuracy (FP16 OK, ~4GB VRAM)
#
# STREAMING (Live Captions): Uses TDT for low-latency real-time transcription
# OFFLINE (Audio Notes):     Uses RNNT for highest accuracy on recordings

services:
  # Ollama Service: Local LLM runtime (auto-pulls models on startup)
  ollama:
    image: ollama/ollama:latest  # Official Ollama image (use 'latest' for latest stable build)
    container_name: ollama       # Custom container name (easier CLI management)
    ports:
      - "11434:11434"            # Port mapping: [Host Port]:[Container Port] (Ollama's default API port)
    volumes:
      # Persistent volume: Stores downloaded models, chat history, and configurations
      # Survives container restarts/deletions (no need to re-download models)
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_GPU_MEMORY=auto  # Auto-manage GPU memory (unload when idle)
      - OLLAMA_IDLE_TIMEOUT=300  # Unload model after 300 seconds (5 minutes) of inactivity (adjust as needed: 60=1min, 120=2min)

    deploy:
      # GPU configuration (for NVIDIA GPUs only; replaces --gpus all CLI flag)
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all          # Attach all available NVIDIA GPUs
              capabilities: [gpu] # Declare GPU hardware acceleration support
    restart: always              # Auto-restart container on unexpected exit (e.g., crash, system reboot)
    networks:
      - ai-network               # Join custom network for seamless communication with Lobe Chat

  # Lobe Chat Service: High-quality UI for interacting with Ollama/Llama models
  lobe-chat:
    image: lobehub/lobe-chat:latest  # Official Lobe Chat image (高颜值 AI chat UI)
    container_name: lobe-chat        # Custom container name
    ports:
      - "3210:3210"                  # Port mapping: Lobe Chat's default web access port
    volumes:
      # Persistent volume: Stores chat history, UI settings, and model configurations
      - lobe-chat-data:/app/data
    environment:
      - LOCALE=zh-CN                # Set UI language to Simplified Chinese (change to 'en-US' for English)
    restart: always                  # Auto-restart on unexpected exit
    networks:
      - ai-network                   # Communicate with Ollama via service name: http://ollama:11434

  # Jupyter PySpark Notebook Service: Interactive Jupyter environment with PySpark, PyTorch, and Transformers
  pyspark-notebook:
    build:
      context: ./services/pyspark
      dockerfile: Dockerfile
    image: pyspark-notebook-ml:latest               # Tag for the built image
    container_name: pyspark-notebook                # Custom container name
    ports:
      - "8888:8888"                                 # Port mapping: Jupyter Notebook web interface
    volumes:
      # Persistent volume: Stores notebooks, data files, workspace, and models
      - pyspark-notebook-data:/home/jovyan/work
    environment:
      - JUPYTER_ENABLE_LAB=yes                      # Enable JupyterLab interface (modern UI)
      - JUPYTER_TOKEN=ai-tools-dev-2025             # Fixed token for easy VS Code connection
    deploy:
      # GPU configuration (for NVIDIA GPUs only; enables PyTorch CUDA)
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all          # Attach all available NVIDIA GPUs
              capabilities: [gpu] # Declare GPU hardware acceleration support
    restart: always                                 # Auto-restart on unexpected exit
    networks:
      - ai-network                                  # Join AI network for potential integration

  # Vosk ASR: Lightweight, fast, offline speech recognition with native streaming (CPU)
  vosk-asr:
    build:
      context: ./services/vosk
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
      args:
        APT_MIRROR: ${APT_MIRROR}
        PYPI_MIRROR: ${PYPI_MIRROR}
        PYPI_TRUSTED_HOST: ${PYPI_TRUSTED_HOST}
        HF_ENDPOINT: ${HF_ENDPOINT}
    image: vosk-asr:latest
    container_name: vosk-asr
    depends_on:
      - text-refiner
    ports:
      - "8001:8000"                                 # Port mapping: Vosk ASR API endpoint
    volumes:
      - vosk-model-data:/app/model                  # Persistent model storage (1.8GB, downloads once)
      - vosk-hf-cache:/root/.cache/huggingface     # HuggingFace cache for punctuation model
    environment:
      - CUDA_VISIBLE_DEVICES=0                      # Use GPU 0 for punctuation model
      - TEXT_REFINER_URL=http://text-refiner:8000   # Text refiner service for punctuation + correction
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1                              # Use 1 GPU for punctuation model
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s                             # Allow time for model loading (pre-downloaded)

  # Parakeet ASR: NVIDIA NeMo GPU-accelerated speech recognition (high accuracy)
  parakeet-asr:
    build:
      context: ./services/parakeet
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
      args:
        APT_MIRROR: ${APT_MIRROR}
        PYPI_MIRROR: ${PYPI_MIRROR}
        PYPI_TRUSTED_HOST: ${PYPI_TRUSTED_HOST}
        HF_ENDPOINT: ${HF_ENDPOINT}
        PYTORCH_CUDA_MIRROR: ${PYTORCH_CUDA_MIRROR}
    image: parakeet-asr:latest
    container_name: parakeet-asr
    depends_on:
      - text-refiner
    ports:
      - "8002:8000"                                 # Port mapping: Parakeet ASR API endpoint
    volumes:
      - parakeet-model-data:/root/.cache/huggingface  # Cache HuggingFace/NGC models (15GB cached)
    environment:
      # Available models:
      #   nvidia/parakeet-tdt-1.1b   - TDT: Fastest, good for real-time (FP16 OK, ~4GB VRAM)
      #   nvidia/parakeet-rnnt-1.1b  - RNNT: Best accuracy, slower (FP32 supported, ~8GB VRAM)
      #   nvidia/parakeet-ctc-1.1b   - CTC: Balanced speed/accuracy (FP16 OK, ~4GB VRAM)
      # Model selection: TDT for streaming (Live Captions), RNNT for offline (Audio Notes)
      - PARAKEET_STREAMING_MODEL=nvidia/parakeet-tdt-1.1b
      - PARAKEET_OFFLINE_MODEL=nvidia/parakeet-rnnt-1.1b
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_JIT=0                               # Fix SIGSEGV in NeMo with PyTorch 2.9.x
      - TEXT_REFINER_URL=http://text-refiner:8000   # Text refiner service for punctuation + correction
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s                            # Allow time for model loading (pre-downloaded)

  # Whisper ASR: OpenAI Whisper Large V3 - GPU-accelerated speech recognition
  # Dual models: Turbo for streaming (fast), Large-v3 for offline (accurate)
  whisper-asr:
    build:
      context: ./services/whisper
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
      args:
        APT_MIRROR: ${APT_MIRROR}
        PYPI_MIRROR: ${PYPI_MIRROR}
        PYPI_TRUSTED_HOST: ${PYPI_TRUSTED_HOST}
        HF_ENDPOINT: ${HF_ENDPOINT}
        PYTORCH_CUDA_MIRROR: ${PYTORCH_CUDA_MIRROR}
    image: whisper-asr:latest
    container_name: whisper-asr
    ports:
      - "8003:8000"                                 # Port mapping: Whisper ASR API endpoint
    volumes:
      - whisper-hf-cache:/root/.cache/huggingface  # Cache HuggingFace models (~13GB for both)
      - whisper-torch-cache:/root/.cache/torch     # Cache Silero VAD model (~100MB)
    environment:
      - WHISPER_MODEL=openai/whisper-large-v3-turbo
      - HF_HUB_OFFLINE=1                           # Force offline mode to prevent network requests
      - CUDA_VISIBLE_DEVICES=0
      # Performance settings (configured in .env)
      - WHISPER_VAD_FILTER=${WHISPER_VAD_FILTER}
      - WHISPER_VAD_THRESHOLD=${WHISPER_VAD_THRESHOLD}
      - WHISPER_BEAM_SIZE=${WHISPER_BEAM_SIZE}
      - WHISPER_LANGUAGE=${WHISPER_LANGUAGE}
      - WHISPER_CHUNK_DURATION_SEC=${WHISPER_CHUNK_DURATION_SEC}
      - WHISPER_MIN_AUDIO_SEC=${WHISPER_MIN_AUDIO_SEC}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s                            # Allow time for model loading (pre-downloaded)

  # FastConformer ASR: NVIDIA FastConformer Hybrid - Optimized streaming ASR
  fastconformer-asr:
    build:
      context: ./services/fastconformer
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
      args:
        APT_MIRROR: ${APT_MIRROR}
        PYPI_MIRROR: ${PYPI_MIRROR}
        PYPI_TRUSTED_HOST: ${PYPI_TRUSTED_HOST}
        PYTORCH_CUDA_MIRROR: ${PYTORCH_CUDA_MIRROR}
    image: fastconformer-asr:latest
    container_name: fastconformer-asr
    ports:
      - "8004:8000"                                 # Port mapping: FastConformer ASR API endpoint
    volumes:
      - fastconformer-model-data:/root/.cache/huggingface  # Cache HuggingFace/NeMo models
      - fastconformer-nemo-cache:/root/.cache/nemo         # NeMo-specific cache
    environment:
      - FASTCONFORMER_MODEL=nvidia/stt_en_fastconformer_hybrid_large_streaming_multi
      - DECODER_TYPE=rnnt                           # rnnt or ctc (rnnt has better accuracy)
      - ATT_CONTEXT_SIZE=[70,6]                     # [70,0]=0ms, [70,1]=80ms, [70,6]=480ms, [70,33]=1040ms
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_JIT=0                               # Fix SIGSEGV in NeMo with PyTorch 2.9.x
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s                            # Allow time for model loading (pre-downloaded)

  # Text Refiner Service: Punctuation + ASR Error Correction
  text-refiner:
    build:
      context: ./services/text-refiner
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
      args:
        APT_MIRROR: ${APT_MIRROR}
        PYPI_MIRROR: ${PYPI_MIRROR}
        PYPI_TRUSTED_HOST: ${PYPI_TRUSTED_HOST}
        HF_ENDPOINT: ${HF_ENDPOINT}
        PYTORCH_CUDA_MIRROR: ${PYTORCH_CUDA_MIRROR}
    image: text-refiner:latest
    container_name: text-refiner
    ports:
      - "8010:8000"                                 # Port mapping: Text Refiner API endpoint
    volumes:
      - text-refiner-hf-cache:/root/.cache/huggingface  # Cache HuggingFace models (T5, etc.)
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s                             # Allow time for model download

  # Audio Notes Service: Gradio Web UI for Transcription & AI Summarization
  audio-notes:
    build:
      context: ./services/audio-notes
      dockerfile: Dockerfile
    image: audio-notes:latest
    container_name: audio-notes
    depends_on:
      - parakeet-asr
      - ollama
    ports:
      - "7860:7860"                                 # Port mapping: Gradio web interface
    volumes:
      - audio-notes-data:/app/recordings              # Persistent storage for audio recordings
    environment:
      - PARAKEET_URL=http://parakeet-asr:8000         # Primary ASR backend
      - WHISPER_URL=http://whisper-asr:8000           # Fallback ASR backend
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen3:14b}
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# Persistent Volumes: Separate container lifecycle from data storage
# Data remains intact even if containers are deleted/rebuilt
volumes:
  ollama-data:                # Stores Ollama's models, chat logs, and runtime data
  lobe-chat-data:             # Stores Lobe Chat's user preferences and chat history
  audio-notes-data:           # Stores audio recordings from Live Captions
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/.recordings
  pyspark-notebook-data:      # Stores Jupyter notebooks, workspace files, and HuggingFace models (Qwen, etc.)
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/notebooks
  vosk-model-data:            # Stores Vosk speech recognition model (1.8GB, pre-downloaded via download_model.sh)
  vosk-hf-cache:              # Stores HuggingFace punctuation model cache for Vosk (~2GB)
  parakeet-model-data:        # Stores NeMo model cache for Parakeet (15GB, pre-downloaded via download_models.sh)
  whisper-hf-cache:           # Stores HuggingFace model cache for Whisper (~13GB, pre-downloaded via download_models.sh)
  whisper-torch-cache:        # Stores torch hub cache for Silero VAD model (~100MB)
  fastconformer-model-data:   # Stores HuggingFace/NeMo model cache for FastConformer (~3GB)
  fastconformer-nemo-cache:   # Stores NeMo-specific cache for FastConformer
  text-refiner-hf-cache:      # Stores HuggingFace model cache for Text Refiner (pre-downloaded via download_models.sh)
# Custom Network: Enables service-to-service communication using container names (instead of IPs)
# Bridge driver is Docker's default for local network isolation
networks:
  ai-network:
    driver: bridge