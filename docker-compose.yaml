# Docker Compose configuration for AI-Tools
# Define all services (containers) to be orchestrated

# ============== MODEL CONFIGURATION ==============
# Available Parakeet models (change here to switch):
#   nvidia/parakeet-tdt-1.1b   - TDT: Fastest, good for real-time (FP16 OK, ~4GB VRAM)
#   nvidia/parakeet-rnnt-1.1b  - RNNT: Best accuracy, slower (FP32 required, ~8GB VRAM)
#   nvidia/parakeet-ctc-1.1b   - CTC: Balanced speed/accuracy (FP16 OK, ~4GB VRAM)
x-parakeet-config: &parakeet-config
  model: nvidia/parakeet-tdt-1.1b
  fp16: "true"  # Set to "true" for TDT/CTC, "false" for RNNT

services:
  # Ollama Service: Local LLM runtime (auto-pulls models on startup)
  ollama:
    image: ollama/ollama:latest  # Official Ollama image (use 'latest' for latest stable build)
    container_name: ollama       # Custom container name (easier CLI management)
    ports:
      - "11434:11434"            # Port mapping: [Host Port]:[Container Port] (Ollama's default API port)
    volumes:
      # Persistent volume: Stores downloaded models, chat history, and configurations
      # Survives container restarts/deletions (no need to re-download models)
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_GPU_MEMORY=auto  # Auto-manage GPU memory (unload when idle)
      - OLLAMA_IDLE_TIMEOUT=300  # Unload model after 300 seconds (5 minutes) of inactivity (adjust as needed: 60=1min, 120=2min)

    deploy:
      # GPU configuration (for NVIDIA GPUs only; replaces --gpus all CLI flag)
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all          # Attach all available NVIDIA GPUs
              capabilities: [gpu] # Declare GPU hardware acceleration support
    restart: always              # Auto-restart container on unexpected exit (e.g., crash, system reboot)
    networks:
      - ai-network               # Join custom network for seamless communication with Lobe Chat

  # Lobe Chat Service: High-quality UI for interacting with Ollama/Llama models
  lobe-chat:
    image: lobehub/lobe-chat:latest  # Official Lobe Chat image (高颜值 AI chat UI)
    container_name: lobe-chat        # Custom container name
    ports:
      - "3210:3210"                  # Port mapping: Lobe Chat's default web access port
    volumes:
      # Persistent volume: Stores chat history, UI settings, and model configurations
      - lobe-chat-data:/app/data
    environment:
      - LOCALE=zh-CN                # Set UI language to Simplified Chinese (change to 'en-US' for English)
    restart: always                  # Auto-restart on unexpected exit
    networks:
      - ai-network                   # Communicate with Ollama via service name: http://ollama:11434

  # Jupyter PySpark Notebook Service: Interactive Jupyter environment with PySpark, PyTorch, and Transformers
  pyspark-notebook:
    build:
      context: ./services/pyspark
      dockerfile: Dockerfile
    image: pyspark-notebook-ml:latest               # Tag for the built image
    container_name: pyspark-notebook                # Custom container name
    ports:
      - "8888:8888"                                 # Port mapping: Jupyter Notebook web interface
    volumes:
      # Persistent volume: Stores notebooks, data files, workspace, and models
      - pyspark-notebook-data:/home/jovyan/work
      # Optional: Uncomment to sync local notebooks folder with Docker
      # - ./notebooks:/home/jovyan/work/notebooks
    environment:
      - JUPYTER_ENABLE_LAB=yes                      # Enable JupyterLab interface (modern UI)
      - JUPYTER_TOKEN=ai-tools-dev-2025             # Fixed token for easy VS Code connection
    deploy:
      # GPU configuration (for NVIDIA GPUs only; enables PyTorch CUDA)
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all          # Attach all available NVIDIA GPUs
              capabilities: [gpu] # Declare GPU hardware acceleration support
    restart: always                                 # Auto-restart on unexpected exit
    networks:
      - ai-network                                  # Join AI network for potential integration

  # Vosk ASR: Lightweight, fast, offline speech recognition with native streaming (CPU)
  vosk-asr:
    build:
      context: ./services/vosk
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
    image: vosk-asr:latest
    container_name: vosk-asr
    depends_on:
      - text-refiner
    ports:
      - "8001:8000"                                 # Port mapping: Vosk ASR API endpoint
    volumes:
      - vosk-model-data:/app/model                  # Persistent model storage (1.8GB, downloads once)
      - vosk-hf-cache:/root/.cache/huggingface     # HuggingFace cache for punctuation model
    environment:
      - CUDA_VISIBLE_DEVICES=0                      # Use GPU 0 for punctuation model
      - TEXT_REFINER_URL=http://text-refiner:8000   # Text refiner service for punctuation + correction
      - ENABLE_TEXT_REFINER=${ENABLE_TEXT_REFINER:-true}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1                              # Use 1 GPU for punctuation model
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s                             # Allow time for first-time model download

  # Parakeet ASR: NVIDIA NeMo GPU-accelerated speech recognition (high accuracy)
  parakeet-asr:
    build:
      context: ./services/parakeet
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
    image: parakeet-asr:latest
    container_name: parakeet-asr
    depends_on:
      - text-refiner
    ports:
      - "8002:8000"                                 # Port mapping: Parakeet ASR API endpoint
    volumes:
      - parakeet-model-data:/root/.cache/huggingface  # Cache HuggingFace/NGC models (15GB cached)
    environment:
      - PARAKEET_MODEL=${PARAKEET_MODEL:-nvidia/parakeet-tdt-1.1b}
      - USE_FP16=${PARAKEET_FP16:-true}
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_JIT=0                               # Fix SIGSEGV in NeMo with PyTorch 2.9.x
      - TEXT_REFINER_URL=http://text-refiner:8000   # Text refiner service for punctuation + correction
      - ENABLE_TEXT_REFINER=${ENABLE_TEXT_REFINER:-true}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s                            # Allow time for model download on first run

  # Whisper ASR: OpenAI Whisper Large V3 Turbo - fast, accurate speech recognition (GPU)
  whisper-asr:
    build:
      context: ./services/whisper
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
    image: whisper-asr:latest
    container_name: whisper-asr
    depends_on:
      - text-refiner
    ports:
      - "8003:8000"                                 # Port mapping: Whisper ASR API endpoint
    volumes:
      - whisper-hf-cache:/root/.cache/huggingface  # Cache HuggingFace models (~3GB)
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-openai/whisper-large-v3-turbo}
      - USE_FLASH_ATTENTION=${WHISPER_FLASH_ATTENTION:-true}
      - CUDA_VISIBLE_DEVICES=0
      - TEXT_REFINER_URL=http://text-refiner:8000   # Text refiner service for punctuation + correction
      - ENABLE_TEXT_REFINER=${ENABLE_TEXT_REFINER:-true}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s                            # Allow time for model download on first run

  # Text Refiner Service: Punctuation + ASR Error Correction
  text-refiner:
    build:
      context: ./services/text-refiner
      dockerfile: Dockerfile
      additional_contexts:
        shared: ./shared                         # Shared Python modules
    image: text-refiner:latest
    container_name: text-refiner
    ports:
      - "8010:8000"                                 # Port mapping: Text Refiner API endpoint
    volumes:
      - text-refiner-hf-cache:/root/.cache/huggingface  # Cache HuggingFace models (T5, etc.)
    environment:
      - PUNCTUATION_MODEL=pcs_en                    # Punctuation ONNX model
      - CORRECTION_MODEL=${CORRECTION_MODEL:-oliverguhr/spelling-correction-english-base}
      - ENABLE_CORRECTION=${ENABLE_CORRECTION:-true}
      - CORRECTION_MIN_WORDS=4                      # Min words before applying correction
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s                             # Allow time for model download

# Note: audio-summary service removed - use local app (apps/audio-notes) instead
# The local app calls whisper-asr and ollama services directly

  # Audio Notes Service: Gradio Web UI for Transcription & AI Summarization
  audio-notes:
    build:
      context: ./services/audio-notes
      dockerfile: Dockerfile
    image: audio-notes:latest
    container_name: audio-notes
    depends_on:
      - whisper-asr
      - ollama
    ports:
      - "7860:7860"                                 # Port mapping: Gradio web interface
    volumes:
      - audio-notes-data:/app/recordings           # Persistent storage for recordings & transcripts
    environment:
      - WHISPER_URL=http://whisper-asr:8000
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen3:14b}
      - RECORDINGS_DIR=/app/recordings
    restart: always
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# Persistent Volumes: Separate container lifecycle from data storage
# Data remains intact even if containers are deleted/rebuilt
volumes:
  ollama-data:                # Stores Ollama's models, chat logs, and runtime data
  lobe-chat-data:             # Stores Lobe Chat's user preferences and chat history
  pyspark-notebook-data:      # Stores Jupyter notebooks, workspace files, and HuggingFace models (Qwen, etc.)
  vosk-model-data:            # Stores Vosk speech recognition model (1.8GB, persists across rebuilds)
  vosk-hf-cache:              # Stores HuggingFace punctuation model cache for Vosk (~2GB)
  parakeet-model-data:        # Stores HuggingFace/NeMo model cache for Parakeet (15GB cached)
  whisper-hf-cache:           # Stores HuggingFace model cache for Whisper (~3GB)
  text-refiner-hf-cache:      # Stores HuggingFace model cache for Text Refiner (T5, etc.)
  audio-notes-data:           # Stores audio recordings and transcripts

# Custom Network: Enables service-to-service communication using container names (instead of IPs)
# Bridge driver is Docker's default for local network isolation
networks:
  ai-network:
    driver: bridge